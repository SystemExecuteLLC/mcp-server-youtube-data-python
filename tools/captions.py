"""
Tools for handling YouTube video captions/subtitles.
"""

import os
import sys
import re
import httpx
from typing import Any, Dict, List
from collections import Counter

from api_client import make_youtube_request, get_oauth_credentials
from utils import safe_get, parse_srt_captions
from constants import YOUTUBE_API_BASE, USER_AGENT

async def get_captions(video_id: str, language_code: str = None, format_type: str = "text") -> Dict[str, Any]:
    """Get captions/subtitles for a YouTube video.
    
    This function retrieves caption tracks for a video and returns either the list of available
    captions or the content of a specific caption track. Caption content can be returned in
    different formats including plain text, SRT, or WEBVTT.
    
    Args:
        video_id: The ID of the YouTube video
        language_code: ISO 639-1 language code (e.g., 'en', 'es', 'fr') for the desired caption track.
                      If None, returns a list of all available caption tracks.
        format_type: Format to return captions in - 'text' (plain text), 'srt' (SubRip), or 'vtt' (WebVTT)
                    Only used when a specific language_code is provided.
    
    Returns:
        Dict containing caption information or error
    """
    # First, get the list of caption tracks for the video
    params = {
        "part": "snippet",
        "videoId": video_id
    }
    
    data = await make_youtube_request("captions", params)
    
    if "error" in data:
        return {"error": data["error"]}
    
    if "items" not in data or not data["items"]:
        return {"error": "No captions found for this video."}
    
    # If no language specified, return the list of available caption tracks
    if language_code is None:
        tracks = []
        for item in data["items"]:
            track_id = safe_get(item, "id", default="Unknown")
            snippet = safe_get(item, "snippet", default={})
            language = safe_get(snippet, "language", default="Unknown")
            name = safe_get(snippet, "name", default="")
            track_type = safe_get(snippet, "trackType", default="Unknown")
            is_auto = track_type == "ASR"  # ASR means auto-generated by YouTube
            
            track_info = f"{language} ({name})" if name else language
            track_info += " (auto-generated)" if is_auto else ""
            
            tracks.append(f"{track_info} - ID: {track_id}")
        
        return {"result": f"Available caption tracks for video {video_id}:\n" + "\n".join(tracks)}
    
    # Find the requested caption track by language code
    caption_id = None
    for item in data["items"]:
        snippet = safe_get(item, "snippet", default={})
        if safe_get(snippet, "language", default="") == language_code:
            caption_id = safe_get(item, "id", default=None)
            break
    
    if not caption_id:
        return {"error": f"No caption track found for language '{language_code}'"}
    
    # Get the caption content
    # Determine format parameter
    format_param = "fmt="
    if format_type.lower() == "srt":
        format_param += "srt"
    elif format_type.lower() == "vtt":
        format_param += "vtt"
    else:  # Default to plain text
        format_param += "text"
    
    # Get API key and OAuth token
    from utils import get_api_key
    api_key = get_api_key()
    oauth_token = await get_oauth_credentials()
    
    if not oauth_token:
        return {
            "error": "OAuth token required to access caption content. "
                    "Set YOUTUBE_OAUTH_TOKEN environment variable."
        }
    
    # Construct the caption download URL
    caption_url = f"{YOUTUBE_API_BASE}/captions/{caption_id}?{format_param}&key={api_key}"
    
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "*/*",
        "Authorization": f"Bearer {oauth_token}"
    }
    
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(caption_url, headers=headers, timeout=30.0)
            response.raise_for_status()
            
            # Process caption content based on format requested
            caption_content = response.text
            
            # For plain text format, clean it up for better readability
            if format_type.lower() == "text":
                # Simple processing to make plain text more readable
                lines = caption_content.split("\n")
                processed_lines = []
                
                for line in lines:
                    # Remove timestamps and other formatting if present
                    if (line.strip() and 
                        not line.strip()[0].isdigit() and 
                        "-->" not in line):
                        processed_lines.append(line)
                
                caption_content = "\n".join(processed_lines)
            
            # Format the result
            result = f"Captions for video {video_id} in {language_code}:\n\n"
            result += caption_content
            
            return {"result": result}
            
        except httpx.HTTPStatusError as e:
            error_message = f"HTTP error {e.response.status_code}: {e.response.reason_phrase}"
            # If we get a 403, it likely means we don't have rights to access this caption
            if e.response.status_code == 403:
                error_message = "Access denied. You may not have permission to access this caption track."
            return {"error": error_message}
            
        except Exception as e:
            return {"error": f"Error retrieving caption content: {str(e)}"}

async def analyze_captions(video_id: str, language_code: str = "en", analysis_type: str = "keywords") -> Dict[str, Any]:
    """Analyze captions/subtitles for a YouTube video.
    
    This function retrieves a specific caption track for a video and performs various types of analysis
    on the content, such as extracting keywords, finding phrases, or generating a timeline breakdown.
    
    Args:
        video_id: The ID of the YouTube video
        language_code: ISO 639-1 language code (e.g., 'en', 'es', 'fr') for the desired caption track
        analysis_type: Type of analysis to perform - 'keywords', 'timeline', or 'phrases'
    
    Returns:
        Dict containing caption analysis or error
    """
    # First, get list of available captions
    params = {
        "part": "snippet",
        "videoId": video_id
    }
    
    data = await make_youtube_request("captions", params)
    
    if "error" in data:
        return {"error": data["error"]}
    
    if "items" not in data or not data["items"]:
        return {"error": "No captions found for this video."}
    
    # Find the requested caption track by language code
    caption_id = None
    for item in data["items"]:
        snippet = safe_get(item, "snippet", default={})
        if safe_get(snippet, "language", default="") == language_code:
            caption_id = safe_get(item, "id", default=None)
            break
    
    if not caption_id:
        return {"error": f"No caption track found for language '{language_code}'"}
    
    # Get the caption content in SRT format for better timestamp handling
    format_param = "fmt=srt"
    
    # Get API key and OAuth token
    from utils import get_api_key
    api_key = get_api_key()
    oauth_token = await get_oauth_credentials()
    
    if not oauth_token:
        return {
            "error": "OAuth token required to access caption content. "
                    "Set YOUTUBE_OAUTH_TOKEN environment variable."
        }
    
    # Construct the caption download URL
    caption_url = f"{YOUTUBE_API_BASE}/captions/{caption_id}?{format_param}&key={api_key}"
    
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "*/*",
        "Authorization": f"Bearer {oauth_token}"
    }
    
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(caption_url, headers=headers, timeout=30.0)
            response.raise_for_status()
            
            # Parse SRT content
            srt_content = response.text
            caption_entries = parse_srt_captions(srt_content)
            
            if not caption_entries:
                return {"error": "Failed to parse caption content."}
            
            # Perform the requested analysis
            if analysis_type.lower() == "keywords":
                return analyze_caption_keywords(caption_entries, video_id, language_code)
            elif analysis_type.lower() == "timeline":
                return analyze_caption_timeline(caption_entries, video_id, language_code)
            elif analysis_type.lower() == "phrases":
                return analyze_caption_phrases(caption_entries, video_id, language_code)
            else:
                return {
                    "error": f"Unknown analysis type: {analysis_type}. "
                            "Use 'keywords', 'timeline', or 'phrases'."
                }
            
        except httpx.HTTPStatusError as e:
            error_message = f"HTTP error {e.response.status_code}: {e.response.reason_phrase}"
            if e.response.status_code == 403:
                error_message = "Access denied. You may not have permission to access this caption track."
            return {"error": error_message}
            
        except Exception as e:
            return {"error": f"Error analyzing caption content: {str(e)}"}

def analyze_caption_keywords(caption_entries: List[Dict[str, Any]], video_id: str, language_code: str) -> Dict[str, Any]:
    """Extract keywords from caption content.
    
    Args:
        caption_entries: List of parsed caption entries
        video_id: YouTube video ID
        language_code: Language code of the captions
        
    Returns:
        Dict with keyword analysis results
    """
    # Combine all caption text
    all_text = " ".join([entry["text"] for entry in caption_entries])
    
    # Clean the text
    # Remove HTML tags
    clean_text = re.sub(r'<[^>]+>', '', all_text)
    # Remove special characters and lowercase
    clean_text = re.sub(r'[^\w\s]', '', clean_text).lower()
    
    # Generate word frequency
    words = clean_text.split()
    
    # Remove common stop words (a basic set, could be expanded)
    stop_words = {
        'a', 'an', 'the', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 
        'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'to', 
        'from', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 
        'further', 'then', 'once', 'here', 'there', 'when', 'where', 
        'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 
        'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 
        'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 
        'will', 'just', 'don', 'should', 'now', 'i', 'me', 'my', 
        'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 
        'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 
        'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 
        'itself', 'they', 'them', 'their', 'theirs', 'themselves', 
        'what', 'which', 'who', 'whom', 'this', 'that', 'these', 
        'those', 'am', 'um', 'uh', 'oh', 'like', 'yeah', 'gonna', 'go', 'get'
    }
    
    filtered_words = [word for word in words if word not in stop_words and len(word) > 1]
    word_freq = Counter(filtered_words)
    
    # Get top keywords
    top_keywords = word_freq.most_common(25)
    
    # Prepare result
    total_duration = caption_entries[-1]["end_seconds"]
    total_minutes = total_duration / 60
    total_word_count = len(words)
    
    result = f"Caption Analysis (Keywords) for video {video_id}:\n\n"
    result += f"Language: {language_code}\n"
    result += f"Duration: {int(total_minutes)} minutes {int(total_duration % 60)} seconds\n"
    result += f"Total Words: {total_word_count}\n"
    result += f"Words Per Minute: {int(total_word_count / total_minutes)}\n\n"
    
    result += "Top Keywords:\n"
    for i, (word, count) in enumerate(top_keywords, 1):
        result += f"{i}. {word}: {count} occurrences\n"
    
    return {"result": result}

def analyze_caption_timeline(caption_entries: List[Dict[str, Any]], video_id: str, language_code: str) -> Dict[str, Any]:
    """Generate a timeline breakdown of caption content.
    
    Args:
        caption_entries: List of parsed caption entries
        video_id: YouTube video ID
        language_code: Language code of the captions
        
    Returns:
        Dict with timeline analysis results
    """
    # Divide the video into segments (e.g., 1-minute segments)
    segment_duration = 60  # 1 minute in seconds
    
    total_duration = caption_entries[-1]["end_seconds"]
    num_segments = int(total_duration / segment_duration) + 1
    
    segments = [[] for _ in range(num_segments)]
    
    # Distribute caption entries into segments
    for entry in caption_entries:
        segment_index = int(entry["start_seconds"] / segment_duration)
        if segment_index < len(segments):
            segments[segment_index].append(entry)
    
    # Generate a summary for each segment
    result = f"Caption Timeline for video {video_id}:\n\n"
    result += f"Language: {language_code}\n"
    result += f"Total Duration: {int(total_duration / 60)} minutes {int(total_duration % 60)} seconds\n\n"
    
    for i, segment in enumerate(segments):
        start_time = i * segment_duration
        end_time = min((i + 1) * segment_duration, total_duration)
        
        # Format timestamps as MM:SS
        start_formatted = f"{int(start_time / 60):02d}:{int(start_time % 60):02d}"
        end_formatted = f"{int(end_time / 60):02d}:{int(end_time % 60):02d}"
        
        if segment:  # Only include non-empty segments
            # Concatenate text from all entries in the segment
            segment_text = " ".join([entry["text"] for entry in segment])
            
            # Truncate if too long and add summary
            if len(segment_text) > 100:
                segment_text = segment_text[:97] + "..."
            
            result += f"{start_formatted} - {end_formatted}: {segment_text}\n\n"
    
    return {"result": result}

def analyze_caption_phrases(caption_entries: List[Dict[str, Any]], video_id: str, language_code: str) -> Dict[str, Any]:
    """Find common phrases and expressions in caption content.
    
    Args:
        caption_entries: List of parsed caption entries
        video_id: YouTube video ID
        language_code: Language code of the captions
        
    Returns:
        Dict with phrase analysis results
    """
    # Combine all caption text
    all_text = " ".join([entry["text"] for entry in caption_entries])
    
    # Clean the text
    clean_text = re.sub(r'<[^>]+>', '', all_text)  # Remove HTML tags
    
    # Extract n-grams (phrases of 2-4 words)
    def get_ngrams(text, n):
        words = re.findall(r'\b\w+\b', text.lower())
        ngrams = []
        
        for i in range(len(words) - n + 1):
            ngram = " ".join(words[i:i+n])
            ngrams.append(ngram)
            
        return ngrams
    
    # Get phrases of different lengths
    bigrams = get_ngrams(clean_text, 2)
    trigrams = get_ngrams(clean_text, 3)
    quadgrams = get_ngrams(clean_text, 4)
    
    # Count frequencies
    bigram_freq = Counter(bigrams)
    trigram_freq = Counter(trigrams)
    quadgram_freq = Counter(quadgrams)
    
    # Filter out common phrases that are likely not meaningful
    stop_bigrams = {
        'of the', 'in the', 'to the', 'on the', 'for the', 'with the', 
        'at the', 'from the', 'by the', 'as the', 'is the', 'to be', 
        'in a', 'is a', 'of a', 'it is', 'this is', 'that is', 
        'there is', 'i think', 'you know'
    }
    
    filtered_bigrams = {
        phrase: count for phrase, count in bigram_freq.items() 
        if phrase not in stop_bigrams and count > 2
    }
    
    filtered_trigrams = {
        phrase: count for phrase, count in trigram_freq.items() 
        if count > 2
    }
    
    filtered_quadgrams = {
        phrase: count for phrase, count in quadgram_freq.items() 
        if count > 2
    }
    
    # Prepare result
    result = f"Caption Analysis (Phrases) for video {video_id}:\n\n"
    result += f"Language: {language_code}\n\n"
    
    # Add most common phrases
    result += "Frequent 2-Word Phrases:\n"
    for i, (phrase, count) in enumerate(
        sorted(filtered_bigrams.items(), key=lambda x: x[1], reverse=True)[:15], 1
    ):
        result += f"{i}. \"{phrase}\" - {count} occurrences\n"
    
    result += "\nFrequent 3-Word Phrases:\n"
    for i, (phrase, count) in enumerate(
        sorted(filtered_trigrams.items(), key=lambda x: x[1], reverse=True)[:10], 1
    ):
        result += f"{i}. \"{phrase}\" - {count} occurrences\n"
    
    result += "\nFrequent 4-Word Phrases:\n"
    for i, (phrase, count) in enumerate(
        sorted(filtered_quadgrams.items(), key=lambda x: x[1], reverse=True)[:5], 1
    ):
        result += f"{i}. \"{phrase}\" - {count} occurrences\n"
    
    return {"result": result}
